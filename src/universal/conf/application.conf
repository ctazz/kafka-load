
read = false

producer {
  properties {
    bootstrap.servers = "10.0.0.39:9093,10.0.0.39:9094"
    bootstrap.servers = ${?BOOTSTRAP_SERVERS}
    client.id = testclient
    key.serializer=org.apache.kafka.common.serialization.StringSerializer
    value.serializer=org.apache.kafka.common.serialization.StringSerializer
    retry.backoff.ms = 10
  }

  topicName = "bpm_in"
  topicName = ${?PRODUCER_TOPIC_NAME}
  numRecordsToWrite = 100
  numWriters = 2
  restBetweenWrites = 6
  window.duration.in.millis = 2000

  max.runtime = 20000
  #If numWriters is greater than the number of partitions in the topic, you need to make this false.
  write.to.specific.partition = false

  records.file = path/to/data
  #This is not a general config item.
  #It is instead used only for the current Driver code.
  send.messages.as.single.record = true

}

consumer {
  properties {
    zookeeper.connect = "127.0.0.1:2181"
    zookeeper.connect = ${?ZOOKEEPER_CONNECT}
    group.id = "theGroupIdw"
    client.id = testclientXXYYYwx
    zookeeper.session.timeout.ms = 400
    zookeeper.sync.time.ms = 200
    auto.commit.enable = true
    auto.commit.interval.ms = 1000
    consumer.timeout.ms = 2000
    rebalance.backoff.ms = 10000
    zookeeper.session.timeout.ms = 10000
  }
  topicsAndThreadNums: {
    bpm_out = 2
  }

  window.duration.in.millis = 2000
  max.runtime = 100000
  work.chunk.size = 100
  time.toWait.for.pulls.when.succceeding.in.millis = 600

}